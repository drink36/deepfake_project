import argparse
import torch
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from torch.utils.data import DataLoader
from lightning.pytorch import Trainer
from lightning.pytorch.callbacks import ModelCheckpoint
from data.dataset import DeepfakeDataset, DeepfakeClipDataset
from xception import Xception
from utils import LrLogger, EarlyStoppingLR
from videomae_v2 import DeepfakeVideoMAEV2
parser = argparse.ArgumentParser(description="Classification model training")
parser.add_argument("--data_root", type=str)
parser.add_argument("--train_metadata", type=str, required=True)
parser.add_argument("--val_metadata", type=str, required=True)
parser.add_argument("--batch_size", type=int, default=8) # VideoMAE å¾ˆåƒ VRAMï¼Œå»ºè­°å…ˆæ”¹å° (8 æˆ– 16)
# === 2. åœ¨åƒæ•¸è£¡åŠ å…¥ videomae_v2 ===
parser.add_argument("--model", type=str, choices=["xception", "meso4", "videomae_v2"]) 
parser.add_argument("--gpus", type=int, default=1)
parser.add_argument("--max_epochs", type=int, default=50) # Fine-tuning é€šå¸¸ä¸ç”¨è·‘å¤ªå¤š epoch
parser.add_argument("--num_train", type=int, default=None)
parser.add_argument("--num_val", type=int, default=2000)
parser.add_argument("--precision", default="16-mixed") # å»ºè­°ç”¨ 16-mixed çœé¡¯å­˜
args = parser.parse_args()
torch.set_float32_matmul_precision('high')
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

if __name__ == "__main__":

    # You can fix the random seed if you want reproducible subsets each epoch:
    # torch.manual_seed(42)
    # random.seed(42)

    learning_rate = 1e-4
    gpus = args.gpus
    total_batch_size = args.batch_size * gpus
    # learning_rate = learning_rate * total_batch_size / 4

    # Setup model
    if args.model == "xception":
        model = Xception(learning_rate, distributed=gpus > 1)
    elif args.model == "videomae_v2":
        print("ðŸš€ Initializing VideoMAE V2...")
        model = DeepfakeVideoMAEV2(
            learning_rate=1e-4, 
            freeze_backbone=False, # å…ˆè©¦è©¦çœ‹å…¨é‡å¾®èª¿ï¼Œè·‘ä¸å‹•å†è¨­ True
            distributed=args.gpus > 1   # <--- é—œéµä¿®æ­£ï¼šå‚³å…¥é€™å€‹åƒæ•¸ï¼
        )
        image_size = 224 # VideoMAE V2 å¿…é ˆæ˜¯ 224
        use_clip = True  # æ¨™è¨˜é€™æ˜¯ 3D æ¨¡åž‹
    else:
        raise ValueError(f"Unknown model: {args.model}")

    if use_clip:
        # --- 3D æ¨¡åž‹ç”¨ DeepfakeClipDataset (å 16 å¹€ Clip) ---
        print(f"ðŸ“¦ Using 3D Clip Dataset (Clip Length=16, Size={image_size})")
        
        # è®€å– JSON list (å‡è¨­ä½ æœ‰å¯«å¥½ helper function æˆ–ç›´æŽ¥åœ¨é€™è£¡è®€)
        import json
        with open(args.train_metadata, 'r') as f:
            train_meta = json.load(f)
        with open(args.val_metadata, 'r') as f:
            val_meta = json.load(f)
            
        # é€™è£¡éœ€è¦è½‰æ›æˆ VideoMetadata ç‰©ä»¶ (å‡è¨­ä½ çš„ Dataset æ”¯æ´ç›´æŽ¥å‚³ list)
        from data.dataset import VideoMetadata
        train_meta_obj = [VideoMetadata(**item) for item in train_meta]
        val_meta_obj = [VideoMetadata(**item) for item in val_meta]
        if args.num_train: train_meta_obj = train_meta_obj[:args.num_train]
        if args.num_val: val_meta_obj = val_meta_obj[:args.num_val]

        train_dataset = DeepfakeClipDataset(
            data_root=args.data_root,
            metadata=train_meta_obj,
            clip_len=16,      # VideoMAE æ¨™æº–é•·åº¦
            image_size=image_size,
            take_num=args.num_train,
            mode='train'      # å•Ÿç”¨ Smart Sampling
        )
        
        val_dataset = DeepfakeClipDataset(
            data_root=args.data_root,
            metadata=val_meta_obj,
            clip_len=16,
            image_size=image_size,
            take_num=args.num_val,
            mode='test'       # Validation ç”¨ä¸­å¿ƒè£åˆ‡
        )
        
    else:
        # --- 2D æ¨¡åž‹ç”¨åŽŸæœ¬çš„ Dataset ---
        print(f"ðŸ–¼ï¸ Using 2D Frame Dataset (Size={image_size})")
        train_dataset = DeepfakeDataset(
            data_root=args.data_root,
            json_file=args.train_metadata,
            image_size=image_size,
            take_num=args.num_train
        )
        val_dataset = DeepfakeDataset(
            data_root=args.data_root,
            json_file=args.val_metadata,
            image_size=image_size,
            take_num=args.num_val
        )

    # === 5. Trainer è¨­å®š (ä¿æŒåŽŸæ¨£) ===
    checkpoint_callback = ModelCheckpoint(
        dirpath=f"./ckpt/{args.model}",
        save_last=True,
        filename=args.model + "-{epoch}-{val_loss:.3f}",
        monitor="val_loss",
        mode="min"
    )

    trainer = Trainer(
        precision=args.precision,
        max_epochs=args.max_epochs,
        callbacks=[checkpoint_callback, LrLogger(), EarlyStoppingLR(lr_threshold=1e-7)],
        accelerator="gpu",
        devices=args.gpus,
        strategy="ddp" if args.gpus > 1 else "auto",
        log_every_n_steps=50
    )

    # é–‹å§‹è¨“ç·´
    trainer.fit(
        model,
        train_dataloaders=DataLoader(train_dataset, batch_size=args.batch_size, num_workers=8, pin_memory=True, shuffle=True, persistent_workers=True),
        val_dataloaders=DataLoader(val_dataset, batch_size=args.batch_size, num_workers=8, pin_memory=True, persistent_workers=True)
    )